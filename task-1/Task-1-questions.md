# Q1-Spark和MapReduce的区别

Spark中最核心的概念是RDD(弹性分布式数据集)，近年来，随着数据量的不断增长，分布式集群并行计算(如MapReduce、Dryad等)被广泛运用于处理日益增长的数据。这些设计优秀的计算模型大都具有容错性好、可扩展性强、负载平衡、编程方法简单等优点，从而使得它们受到众多企业的青睐，被大多数用户用来进行大规模数据的处理。 

但是，MapReduce这些并行计算大都是基于非循环的数据流模型，也就是说，一次数据过程包含从共享文件系统读取数据、进行计算、完成计算、写入计算结果到共享存储中，在计算过程中，不同计算节点之间保持高度并行，这样的数据流模型使得那些需要反复使用一个特定数据集的迭代算法无法高效地运行。 

Spark和Spark使用的RDD就是为了解决这种问题而开发出来的，Spark使用了一种特殊设计的数据结构，称为RDD。RDD的一个重要特征是，分布式数据集可以在不同的并行环境当中被重复使用，这个特性将Spark和其他并行数据流模型框架(如MapReduce)区别开

> 原文链接：https://blog.csdn.net/sanqima/article/details/51200880



# Q2-RDD的本质

- 一个RDD就是一个分布式对象集合，本质上是一个只读的分区记录集合，每个RDD可以分成多个分区，每个分区就是一个数据集片段，并且一个RDD的不同分区可以被保存到集群中不同的节点上，从而可以在集群中的不同节点上进行并行计算；
- RDD提供了一种高度受限的共享内存模型，即RDD是只读的记录分区的集合，不能直接修改，只能基于稳定的物理存储中的数据集来创建RDD，或者通过在其他RDD上执行确定的转换操作（如map、join和groupBy）而创建得到新的RDD；
- RDD提供了一组丰富的操作以支持常见的数据运算，分为“行动”（Action）和“转换”（Transformation）两种类型，前者用于执行计算并指定输出的形式，后者指定RDD之间的相互依赖关系。两类操作的主要区别是，转换操作（比如map、filter、groupBy、join等）接受RDD并返回RDD，而行动操作（比如count、collect等）接受RDD但是返回非RDD（即输出一个值或结果）；



# 问题

基本概念介绍得比较清晰，代码方面对于windows用户略不友好

遇到如下问题：

![1565275802297](C:\Users\Cao\AppData\Roaming\Typora\typora-user-images\1565275802297.png)